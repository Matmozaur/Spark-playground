{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, sum, avg, col, to_timestamp, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/09 22:04:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/09 22:04:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f8abc7f8-33e3-4028-84d7-33f77ec5c265. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/09 22:04:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----------+---------+--------+--------+----+\n",
      "|side|product_id|last_size|best_bid|best_ask|time|\n",
      "+----+----------+---------+--------+--------+----+\n",
      "+----+----------+---------+--------+--------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-BTC   |0.02339687|0.08695 |0.08697 |2021-12-09 22:04:34.335|\n",
      "|sell|ETH-USD   |0.21571133|4195.02 |4195.03 |2021-12-09 22:04:34.447|\n",
      "|sell|ETH-USD   |0.38501647|4195.02 |4195.03 |2021-12-09 22:04:34.574|\n",
      "|sell|ETH-USD   |0.17176077|4195.02 |4195.03 |2021-12-09 22:04:34.574|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |2.98995668|4195.02 |4195.03 |2021-12-09 22:04:35.518|\n",
      "|buy |ETH-USD   |1.26187722|4195.02 |4195.03 |2021-12-09 22:04:35.518|\n",
      "|buy |ETH-USD   |0.02649087|4195.02 |4195.04 |2021-12-09 22:04:35.518|\n",
      "|buy |ETH-USD   |1.27594497|4195.02 |4195.24 |2021-12-09 22:04:35.518|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.02340819|4195.23 |4195.24 |2021-12-09 22:04:36.032|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size|best_bid|best_ask|time                   |\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |4.6856E-4|4195.23 |4195.24 |2021-12-09 22:04:36.516|\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |1.03574872|4195.23 |4195.24 |2021-12-09 22:04:36.897|\n",
      "|buy |ETH-USD   |0.1772755 |4195.23 |4195.24 |2021-12-09 22:04:36.897|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.69901506|4195.23 |4195.24 |2021-12-09 22:04:36.969|\n",
      "|buy |ETH-USD   |0.64014586|4195.23 |4195.25 |2021-12-09 22:04:36.973|\n",
      "|buy |ETH-USD   |2.25458354|4195.23 |4195.25 |2021-12-09 22:04:36.973|\n",
      "|buy |ETH-USD   |1.64757953|4195.23 |4195.28 |2021-12-09 22:04:36.973|\n",
      "|buy |ETH-USD   |2.13931706|4195.23 |4195.32 |2021-12-09 22:04:36.973|\n",
      "|buy |ETH-USD   |0.61388908|4195.31 |4195.32 |2021-12-09 22:04:36.985|\n",
      "|buy |ETH-USD   |0.66338357|4195.31 |4195.32 |2021-12-09 22:04:36.996|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |9.53124654|4195.31 |4195.32 |2021-12-09 22:04:37.234|\n",
      "|buy |ETH-USD   |0.05089168|4195.31 |4195.32 |2021-12-09 22:04:37.247|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.40927994|4195.31 |4195.32 |2021-12-09 22:04:37.247|\n",
      "|buy |ETH-USD   |3.67133003|4195.34 |4195.56 |2021-12-09 22:04:37.39 |\n",
      "|buy |ETH-USD   |0.08372966|4195.31 |4195.32 |2021-12-09 22:04:37.247|\n",
      "|buy |ETH-USD   |2.22764642|4195.34 |4195.73 |2021-12-09 22:04:37.39 |\n",
      "|buy |ETH-USD   |0.52153082|4195.31 |4195.32 |2021-12-09 22:04:37.258|\n",
      "|buy |ETH-USD   |2.17890164|4195.34 |4195.75 |2021-12-09 22:04:37.39 |\n",
      "|buy |ETH-USD   |1.03930427|4195.31 |4195.33 |2021-12-09 22:04:37.258|\n",
      "|buy |ETH-USD   |1.11946798|4195.34 |4196.65 |2021-12-09 22:04:37.39 |\n",
      "|buy |ETH-USD   |1.11039588|4195.31 |4195.33 |2021-12-09 22:04:37.262|\n",
      "|buy |ETH-USD   |1.34610112|4195.31 |4195.35 |2021-12-09 22:04:37.262|\n",
      "|buy |ETH-USD   |1.184133  |4195.31 |4195.35 |2021-12-09 22:04:37.262|\n",
      "|buy |ETH-USD   |1.92870428|4195.34 |4195.35 |2021-12-09 22:04:37.294|\n",
      "|buy |ETH-USD   |0.30478   |4195.34 |4195.35 |2021-12-09 22:04:37.301|\n",
      "|buy |ETH-USD   |0.02243647|4195.34 |4195.35 |2021-12-09 22:04:37.328|\n",
      "|buy |ETH-USD   |0.02467784|4195.34 |4195.35 |2021-12-09 22:04:37.338|\n",
      "|buy |ETH-USD   |0.02562307|4195.34 |4195.35 |2021-12-09 22:04:37.349|\n",
      "|buy |ETH-USD   |0.02273962|4195.34 |4195.35 |2021-12-09 22:04:37.365|\n",
      "|buy |ETH-USD   |0.02331842|4195.34 |4195.35 |2021-12-09 22:04:37.381|\n",
      "|buy |ETH-USD   |0.0137    |4195.34 |4195.35 |2021-12-09 22:04:37.381|\n",
      "|buy |ETH-USD   |0.47107916|4195.34 |4195.35 |2021-12-09 22:04:37.383|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                  |\n",
      "+----+----------+----------+--------+--------+----------------------+\n",
      "|buy |ETH-USD   |0.11687838|4196.64 |4196.65 |2021-12-09 22:04:37.67|\n",
      "+----+----------+----------+--------+--------+----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|sell|ETH-BTC   |0.18314842|0.08694 |0.08697 |2021-12-09 22:04:38.136|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.23375676|4196.64 |4196.65 |2021-12-09 22:04:38.576|\n",
      "|buy |ETH-USD   |0.35220941|4196.64 |4196.65 |2021-12-09 22:04:38.616|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |1.43669272|4196.64 |4196.65 |2021-12-09 22:04:38.873|\n",
      "|buy |ETH-USD   |0.35020728|4196.64 |4196.65 |2021-12-09 22:04:38.873|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.02370997|4196.64 |4196.65 |2021-12-09 22:04:38.967|\n",
      "|buy |ETH-USD   |0.95123871|4196.64 |4196.65 |2021-12-09 22:04:38.999|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.09363274|4196.64 |4196.65 |2021-12-09 22:04:39.25 |\n",
      "|buy |ETH-USD   |0.40406888|4196.64 |4196.65 |2021-12-09 22:04:39.253|\n",
      "|buy |ETH-USD   |0.44189586|4196.64 |4196.65 |2021-12-09 22:04:39.266|\n",
      "|buy |ETH-USD   |0.10792829|4196.64 |4196.65 |2021-12-09 22:04:39.266|\n",
      "|buy |ETH-USD   |0.41198363|4196.64 |4196.65 |2021-12-09 22:04:39.266|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.97240623|4196.64 |4196.66 |2021-12-09 22:04:39.273|\n",
      "|buy |ETH-USD   |0.00485498|4196.65 |4196.66 |2021-12-09 22:04:39.28 |\n",
      "|buy |ETH-USD   |0.42087532|4196.65 |4196.66 |2021-12-09 22:04:39.28 |\n",
      "|buy |ETH-USD   |1.53421219|4196.65 |4197.0  |2021-12-09 22:04:39.285|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |1.88638031|4196.99 |4197.03 |2021-12-09 22:04:39.445|\n",
      "|buy |ETH-USD   |0.00241837|4196.99 |4197.16 |2021-12-09 22:04:39.445|\n",
      "|buy |ETH-USD   |3.9166E-4 |4196.99 |4197.36 |2021-12-09 22:04:39.445|\n",
      "|buy |ETH-USD   |0.001     |4196.99 |4197.39 |2021-12-09 22:04:39.445|\n",
      "|buy |ETH-USD   |0.01      |4196.99 |4197.52 |2021-12-09 22:04:39.445|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|sell|ETH-USD   |2.50979841|4197.51 |4197.52 |2021-12-09 22:04:39.702|\n",
      "|sell|ETH-USD   |0.0137    |4197.51 |4197.52 |2021-12-09 22:04:39.71 |\n",
      "|sell|ETH-USD   |3.4302984 |4197.51 |4197.52 |2021-12-09 22:04:39.756|\n",
      "|sell|ETH-USD   |0.2899916 |4197.51 |4197.52 |2021-12-09 22:04:39.756|\n",
      "|sell|ETH-USD   |8.4E-6    |4196.65 |4197.52 |2021-12-09 22:04:39.76 |\n",
      "|buy |ETH-USD   |0.5678    |4196.65 |4196.67 |2021-12-09 22:04:39.789|\n",
      "|buy |ETH-USD   |0.56169   |4196.65 |4197.31 |2021-12-09 22:04:39.789|\n",
      "|buy |ETH-USD   |1.56577164|4196.65 |4197.32 |2021-12-09 22:04:39.789|\n",
      "|buy |ETH-USD   |1.55795295|4196.65 |4197.5  |2021-12-09 22:04:39.789|\n",
      "|sell|ETH-USD   |0.00357427|4196.66 |4197.42 |2021-12-09 22:04:39.84 |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|sell|ETH-USD   |0.28642573|4196.64 |4196.67 |2021-12-09 22:04:39.913|\n",
      "|sell|ETH-USD   |0.43658694|4196.64 |4196.67 |2021-12-09 22:04:39.913|\n",
      "|sell|ETH-USD   |0.05966594|4196.65 |4196.67 |2021-12-09 22:04:39.972|\n",
      "|buy |ETH-USD   |7.1307E-4 |4196.65 |4196.66 |2021-12-09 22:04:39.983|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.02053141|4196.65 |4196.66 |2021-12-09 22:04:40.134|\n",
      "|sell|ETH-USD   |0.23033   |4196.65 |4196.66 |2021-12-09 22:04:40.138|\n",
      "|sell|ETH-USD   |4.06E-6   |4196.64 |4196.66 |2021-12-09 22:04:40.153|\n",
      "|sell|ETH-USD   |1.13840594|4196.64 |4196.66 |2021-12-09 22:04:40.153|\n",
      "|buy |ETH-USD   |0.02301438|4196.1  |4196.11 |2021-12-09 22:04:40.224|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|sell|ETH-BTC   |0.00944638|0.08695 |0.08696 |2021-12-09 22:04:40.666|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size|best_bid|best_ask|time                   |\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.5678   |4196.52 |4197.03 |2021-12-09 22:04:40.868|\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |7.0138E-4 |4196.63 |4196.98 |2021-12-09 22:04:41.196|\n",
      "|sell|ETH-BTC   |0.22250037|0.08694 |0.08697 |2021-12-09 22:04:41.2  |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.47416344|4196.64 |4196.97 |2021-12-09 22:04:41.354|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.01020285|4196.68 |4196.96 |2021-12-09 22:04:41.532|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |1.41283257|4196.95 |4196.98 |2021-12-09 22:04:41.647|\n",
      "|buy |ETH-USD   |2.14976841|4196.95 |4197.52 |2021-12-09 22:04:41.647|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.02709589|4197.51 |4197.52 |2021-12-09 22:04:41.837|\n",
      "|buy |ETH-USD   |0.59368533|4197.51 |4197.52 |2021-12-09 22:04:41.837|\n",
      "|buy |ETH-USD   |2.03364519|4197.51 |4197.83 |2021-12-09 22:04:41.837|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size|best_bid|best_ask|time                   |\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "|sell|ETH-BTC   |9.5282435|0.08694 |0.08696 |2021-12-09 22:04:42.335|\n",
      "+----+----------+---------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|sell|ETH-BTC   |0.20984802|0.08693 |0.08695 |2021-12-09 22:04:42.492|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|side|product_id|last_size |best_bid|best_ask|time                   |\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "|buy |ETH-USD   |0.01542856|4197.8  |4197.81 |2021-12-09 22:04:42.663|\n",
      "+----+----------+----------+--------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.sql.streaming.schemaInference\", True).getOrCreate()\n",
    "\n",
    "stream = spark.\\\n",
    "    readStream.\\\n",
    "    format(\"ws\").\\\n",
    "    option(\"schema\", \"ticker\").\\\n",
    "    load() # we need to pass `option(\"schema\", \"ticker\")` to get correct channel subscribed\n",
    "\n",
    "query = stream.select(\"side\", \"product_id\", \"last_size\", \"best_bid\", \"best_ask\", \"time\").\\\n",
    "    writeStream.\\\n",
    "    format(\"console\").\\\n",
    "    outputMode(\"append\").\\\n",
    "    option(\"truncate\", \"false\").\\\n",
    "    start()\n",
    "\n",
    "query.awaitTermination(10) # 2t's wait for 10 seconds.\n",
    "query.stop() # Let's stop the query\n",
    "# stream.printSchema()\n",
    "#spark.stop() # And stop the whole session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = false)\n",
      " |-- trade_id: long (nullable = false)\n",
      " |-- sequence: long (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- price: double (nullable = false)\n",
      " |-- side: string (nullable = false)\n",
      " |-- last_size: double (nullable = false)\n",
      " |-- best_bid: double (nullable = false)\n",
      " |-- best_ask: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruchamiająć `stream.start()` uruchamiamy w osobnym demonie websocket który streamuje wyniki. Jeżeli wystąpi jakiś błąd po stronie front-endu (np. błąd parsowania kolejnej linijki Pythona) fakt ten nie zostanie zgłoszony do sparka i socket pozostanie otwarty! Należy pamiętać, by zamykać stream za każdym razem używająć metody `stop()` (w powyższym przykładzie `query.stop()`). W przypadku utracenia referencji do zapytania, należy zastopować całą sesję również metodą `stop()` (w powyższym przykładzie `spark.stop()`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panic button - press only if you messed up opening new websocket and lost reference to it\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "\n",
    "**Analiza strumienia danych CoinBase (3p)**. Napisz zapytanie, które wypisuje średnią wartość wybranego parametru (np. `price`) w przesuwnych oknach czasowych względem czasu transakcji (kolumna `time`), grupując po relacji wymiany (z jakiej waluty na jaką walutę - kolumna `product_id`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 22:04:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2dd660b3-da34-4166-a4fe-d8ce15cb087f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/09 22:04:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "21/12/09 22:04:44 WARN WSMicroBatchStreamer: Failed to subscribe to websocket: Attempt to decode value on failed cursor: DownField(channels) \n",
      " message: {\"type\":\"ticker\",\"sequence\":4777311648,\"product_id\":\"ETH-BTC\",\"price\":\"0.08695\",\"open_24h\":\"0.08742\",\"volume_24h\":\"11575.21692042\",\"low_24h\":\"0.08597\",\"high_24h\":\"0.08864\",\"volume_30d\":\"432667.68725007\",\"best_bid\":\"0.08690\",\"best_ask\":\"0.08695\",\"side\":\"buy\",\"time\":\"2021-12-09T22:04:43.623576Z\",\"trade_id\":24311413,\"last_size\":\"0.00117751\"}\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+-------+\n",
      "|window|product_id|mean 5s|\n",
      "+------+----------+-------+\n",
      "+------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|mean 5s          |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|mean 5s          |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08691          |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.810333333333|\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|mean 5s          |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08691          |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.655641025641|\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|mean 5s          |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08691          |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.3038        |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|mean 5s          |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08691          |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.077321428572|\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4195.2266666666665 |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4195.09375         |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4195.085555555556  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4195.256           |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.225217391305  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.0869             |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4196.803333333334  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.0869             |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4197.673333333333  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08691733333333333|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4197.5596774193555 |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4197.189302325582  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4197.03170212766   |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4196.846981132076  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4196.846981132076  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:05:00, 2021-12-09 22:05:05}|ETH-USD   |4195.847142857143  |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4196.846981132076  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:05:00, 2021-12-09 22:05:05}|ETH-USD   |4196.06            |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|window                                    |product_id|mean 5s            |\n",
      "+------------------------------------------+----------+-------------------+\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-BTC   |0.08690500000000001|\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-USD   |4196.846981132076  |\n",
      "|{2021-12-09 22:04:45, 2021-12-09 22:04:50}|ETH-USD   |4198.029298245614  |\n",
      "|{2021-12-09 22:04:50, 2021-12-09 22:04:55}|ETH-USD   |4196.258000000001  |\n",
      "|{2021-12-09 22:04:40, 2021-12-09 22:04:45}|ETH-USD   |4198.663333333333  |\n",
      "|{2021-12-09 22:05:00, 2021-12-09 22:05:05}|ETH-USD   |4196.166875        |\n",
      "|{2021-12-09 22:04:55, 2021-12-09 22:05:00}|ETH-BTC   |0.08695216216216217|\n",
      "+------------------------------------------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 22:05:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2b225e3b is aborting.\n",
      "21/12/09 22:05:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2b225e3b aborted.\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43607,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43613,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43614,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43615,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43598,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43605,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43602,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43603,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43601,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43604,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43606,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43600,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN Shell: Interrupted while joining on: Thread[Thread-43599,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 25 (task 4577, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 29 (task 4581, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 28 (task 4580, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 31 (task 4583, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 26 (task 4578, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 29 (task 4581, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 31 (task 4583, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 29 (task 4581, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 31 (task 4583, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 32 (task 4584, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 28 (task 4580, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 26 (task 4578, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 25 (task 4577, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 25 (task 4577, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 30 (task 4582, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 30 (task 4582, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 26 (task 4578, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 28 (task 4580, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 32 (task 4584, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 30 (task 4582, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 32 (task 4584, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 20 (task 4572, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 20 (task 4572, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 20 (task 4572, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 27.0 in stage 43.0 (TID 4579) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 19.0 in stage 43.0 (TID 4571) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 33 (task 4585, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 24.0 in stage 43.0 (TID 4576) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 33 (task 4585, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 33 (task 4585, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 23.0 in stage 43.0 (TID 4575) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 21.0 in stage 43.0 (TID 4573) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 32.0 in stage 43.0 (TID 4584) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 30.0 in stage 43.0 (TID 4582) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 20.0 in stage 43.0 (TID 4572) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 26.0 in stage 43.0 (TID 4578) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 31.0 in stage 43.0 (TID 4583) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 28.0 in stage 43.0 (TID 4580) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 25.0 in stage 43.0 (TID 4577) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 33.0 in stage 43.0 (TID 4585) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 29.0 in stage 43.0 (TID 4581) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:05:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 34 (task 4586, attempt 0, stage 43.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborting commit for partition 34 (task 4586, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 ERROR DataWritingSparkTask: Aborted commit for partition 34 (task 4586, attempt 0, stage 43.0)\n",
      "21/12/09 22:05:03 WARN TaskSetManager: Lost task 34.0 in stage 43.0 (TID 4586) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = false)\n",
      " |-- trade_id: long (nullable = false)\n",
      " |-- sequence: long (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- price: double (nullable = false)\n",
      " |-- side: string (nullable = false)\n",
      " |-- last_size: double (nullable = false)\n",
      " |-- best_bid: double (nullable = false)\n",
      " |-- best_ask: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.sql.streaming.schemaInference\", True).\\\n",
    "    config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True).getOrCreate()  \n",
    "\n",
    "stream = spark.readStream.\\\n",
    "    format(\"ws\").\\\n",
    "    option(\"schema\", \"ticker\").\\\n",
    "    load()\n",
    "\n",
    "\n",
    "WINDOW = '5'\n",
    "\n",
    "\n",
    "query = stream.select(\"time\", \"product_id\", \"price\").\\\n",
    "    groupBy(window(\"time\", \"{} seconds\".format(WINDOW)), \"product_id\").\\\n",
    "    agg(avg(\"price\").alias(\"mean {}s\".format(WINDOW))).\\\n",
    "    writeStream.\\\n",
    "    outputMode(\"complete\").\\\n",
    "    format(\"console\").\\\n",
    "    option(\"truncate\", \"false\").\\\n",
    "    start()\n",
    "\n",
    "query.awaitTermination(20) \n",
    "query.stop() \n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "\n",
    "**Watermarking i dane opóźnione (3p).** \n",
    "Zmodyfikuj zapytanie z zadania 1 tak, aby zademonstować mechanizm znaków wodnych (watermarks) i obsługi danych opóźnionych. W konsoli powinno być widać, że aktualizują się odpowiednie wiersze tabeli wynikowej (tryb update), w szczególności aktualizacja wcześniejszych okien czasowych po przybyciu danych opóźnionych. **Do rozwiązania tego zadania proszę dołączyć przykładowy output i jego opis wyjaśniający na konkretnym przykładzie działanie znaku wodnego i danych opóźnionych**. \n",
    "\n",
    "Do ćwiczenia można wykorzystać skrypt w katalogu `/mock` napisany w [Scala-cli](https://scala-cli.virtuslab.org), który posłuży jako kontrolowane źródło danych CoinBase przez Websocket. \n",
    "\n",
    "Skrypt można uruchomić wykorzystując Docker:\n",
    "\n",
    "```\n",
    "make image\n",
    "make run\n",
    "```\n",
    "\n",
    "Spowoduje to utworzenie websocketowego serwera pod adresem `ws://mock:8025`\n",
    "\n",
    "Po uruchomieniu serwera należy wykonać poniższą komórkę, w której zapytanie czyta dane z utworzonego websocketa. Skrypt wysyła przykładowe wiadomości w formacie CoinBase co 10 sekund:\n",
    "\n",
    "- W pierwszej serii wysyłane wiadomości o znacznikach czasowych 0s, 14s, 7s  \n",
    "- W drugiej serii wysyłane są wiadomości o znacznikach czasowych 15s, 8s, 21s  \n",
    "- W trzeciej serii wysyłane są wiadomości o znacznikach czasowych 4s, 17s  \n",
    "\n",
    "Dla tych danych można ustawić okno czasowe na interwał 10 sekund. Skrypt można też zmodyfikować, tak aby wysyłał inne dane. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 22:05:03 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a2850f72-e841-4142-9e44-1ee91737fb84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/09 22:05:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+-------+\n",
      "|window|product_id|mean 5s|\n",
      "+------+----------+-------+\n",
      "+------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|mean 5s           |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-01 00:00:05, 2021-11-01 00:00:10}|ETH-USD   |869.5649306044588 |\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:05}|ETH-USD   |779.7320652551889 |\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:15}|ETH-USD   |20.550742869765216|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|mean 5s           |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-01 00:00:05, 2021-11-01 00:00:10}|ETH-USD   |814.5847420120178 |\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:05}|ETH-USD   |779.7320652551889 |\n",
      "|{2021-11-01 00:00:20, 2021-11-01 00:00:25}|ETH-USD   |177.92235128905077|\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:15}|ETH-USD   |20.550742869765216|\n",
      "|{2021-11-01 00:00:15, 2021-11-01 00:00:20}|ETH-USD   |768.4917991986489 |\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|mean 5s           |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-01 00:00:05, 2021-11-01 00:00:10}|ETH-USD   |814.5847420120178 |\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:05}|ETH-USD   |887.9666678543429 |\n",
      "|{2021-11-01 00:00:20, 2021-11-01 00:00:25}|ETH-USD   |177.92235128905077|\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:15}|ETH-USD   |20.550742869765216|\n",
      "|{2021-11-01 00:00:15, 2021-11-01 00:00:20}|ETH-USD   |622.0883364151372 |\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n",
      "root\n",
      " |-- type: string (nullable = false)\n",
      " |-- trade_id: long (nullable = false)\n",
      " |-- sequence: long (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- price: double (nullable = false)\n",
      " |-- side: string (nullable = false)\n",
      " |-- last_size: double (nullable = false)\n",
      " |-- best_bid: double (nullable = false)\n",
      " |-- best_ask: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.sql.streaming.schemaInference\", True).getOrCreate()\n",
    "\n",
    "# Note url option!\n",
    "stream = spark.readStream.\\\n",
    "    format(\"ws\").\\\n",
    "    option(\"schema\", \"ticker\").\\\n",
    "    option(\"url\", \"ws://mock:8025\").load() # we pass explicit url option to subscribe to our mock service\n",
    "\n",
    "\n",
    "WINDOW = '5'\n",
    "\n",
    "\n",
    "query = stream.select(\"time\", \"product_id\", \"price\").\\\n",
    "    withWatermark(\"time\", \"30 seconds\").\\\n",
    "    groupBy(window(\"time\", \"{} seconds\".format(WINDOW)), \"product_id\").\\\n",
    "    agg(avg(\"price\").alias(\"mean {}s\".format(WINDOW))).\\\n",
    "    writeStream.\\\n",
    "    outputMode(\"complete\").\\\n",
    "    format(\"console\").\\\n",
    "    option(\"truncate\", \"false\").\\\n",
    "    start()\n",
    "\n",
    "query.awaitTermination(60) \n",
    "query.stop() \n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "\n",
    "**Łączenie strumieni (3p)**. Rozdziel sztucznie dane CoinBase z kanału `ticker` na dwa strumienie (wykorzystując filtrowanie subskrypcji): jeden strumień dla `side=\"sell\"`, drugi dla `side=\"buy\"`. Następnie stwórz zapytanie, które łączy te strumienie i wypisuje transakcje dla danego `product_id`, które występowały po sobie w ciągu 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 22:06:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e2be3795-aa89-4d14-b319-b0a2144362fb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/09 22:06:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----+----------+-----+----+----+----------+-----+\n",
      "|side|time|product_id|price|side|time|product_id|price|\n",
      "+----+----+----------+-----+----+----+----------+-----+\n",
      "+----+----+----------+-----+----+----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|side|time                   |product_id|price  |side|time                   |product_id|price  |\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|sell|2021-12-09 22:06:06.02 |ETH-USD   |4198.01|buy |2021-12-09 22:06:06.062|ETH-USD   |4197.81|\n",
      "|sell|2021-12-09 22:06:06.394|ETH-USD   |4197.8 |buy |2021-12-09 22:06:06.062|ETH-USD   |4197.81|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.11|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.09|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.16|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.15|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.11|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.09|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.16|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.15|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.14|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.11|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.09|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.16|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:07.97 |ETH-USD   |4198.15|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|side|time                   |product_id|price  |side|time                   |product_id|price  |\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.644|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.68 |ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.754|ETH-USD   |4198.14|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.97 |ETH-USD   |4198.25|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.758|ETH-USD   |4198.14|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:08.023|ETH-USD   |4198.15|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.833|ETH-USD   |4198.14|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:08.031|ETH-USD   |4198.15|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.844|ETH-USD   |4198.15|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:08.035|ETH-USD   |4198.15|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.86 |ETH-USD   |4198.15|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.469|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.86 |ETH-USD   |4198.25|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.478|ETH-USD   |4198.11|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.898|ETH-USD   |4198.24|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.493|ETH-USD   |4198.19|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.898|ETH-USD   |4198.24|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.511|ETH-USD   |4198.19|\n",
      "|sell|2021-12-09 22:06:08.264|ETH-USD   |4198.15|buy |2021-12-09 22:06:07.901|ETH-USD   |4198.24|\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|side|time                   |product_id|price  |side|time                   |product_id|price  |\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08686|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08686|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:10.673|ETH-BTC   |0.08686|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:11.374|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:11.374|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:11.573|ETH-BTC   |0.08684|buy |2021-12-09 22:06:11.374|ETH-BTC   |0.08685|\n",
      "|sell|2021-12-09 22:06:10.71 |ETH-USD   |4197.8 |buy |2021-12-09 22:06:11.367|ETH-USD   |4196.79|\n",
      "|sell|2021-12-09 22:06:10.711|ETH-USD   |4197.8 |buy |2021-12-09 22:06:11.367|ETH-USD   |4196.79|\n",
      "|sell|2021-12-09 22:06:10.713|ETH-USD   |4197.8 |buy |2021-12-09 22:06:11.367|ETH-USD   |4196.79|\n",
      "|sell|2021-12-09 22:06:10.718|ETH-USD   |4197.8 |buy |2021-12-09 22:06:11.367|ETH-USD   |4196.79|\n",
      "|sell|2021-12-09 22:06:10.719|ETH-USD   |4197.8 |buy |2021-12-09 22:06:11.367|ETH-USD   |4196.79|\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|side|time                   |product_id|price  |side|time                   |product_id|price  |\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|sell|2021-12-09 22:06:13.736|ETH-USD   |4195.79|buy |2021-12-09 22:06:12.875|ETH-USD   |4196.66|\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.79|buy |2021-12-09 22:06:12.875|ETH-USD   |4196.66|\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.69|buy |2021-12-09 22:06:12.875|ETH-USD   |4196.66|\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.69|buy |2021-12-09 22:06:12.875|ETH-USD   |4196.66|\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.38|buy |2021-12-09 22:06:12.875|ETH-USD   |4196.66|\n",
      "|sell|2021-12-09 22:06:13.033|ETH-USD   |4196.11|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.101|ETH-USD   |4196.11|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.736|ETH-USD   |4195.79|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.79|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.69|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.69|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.38|buy |2021-12-09 22:06:13.813|ETH-USD   |4195.8 |\n",
      "|sell|2021-12-09 22:06:13.033|ETH-USD   |4196.11|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.101|ETH-USD   |4196.11|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.736|ETH-USD   |4195.79|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.79|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.792|ETH-USD   |4195.69|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.69|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.841|ETH-USD   |4195.38|buy |2021-12-09 22:06:14.005|ETH-USD   |4195.06|\n",
      "|sell|2021-12-09 22:06:13.736|ETH-USD   |4195.79|buy |2021-12-09 22:06:14.164|ETH-USD   |4195.02|\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|side|time                   |product_id|price  |side|time                   |product_id|price  |\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "|sell|2021-12-09 22:06:18.357|ETH-USD   |4196.4 |buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4195.01|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4193.84|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4196.37|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4195.0 |buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4193.84|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4196.36|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4194.87|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4193.83|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4195.99|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4194.76|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4193.81|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4195.98|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4194.35|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.374|ETH-USD   |4195.99|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4195.97|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4194.34|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.374|ETH-USD   |4193.81|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.361|ETH-USD   |4195.97|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "|sell|2021-12-09 22:06:18.365|ETH-USD   |4194.33|buy |2021-12-09 22:06:18.686|ETH-USD   |4196.34|\n",
      "+----+-----------------------+----------+-------+----+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 22:06:25 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4b5d784d is aborting.\n",
      "21/12/09 22:06:25 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4b5d784d aborted.\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106432,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106433,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106434,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106435,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106437,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106436,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106438,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106442,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106443,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106444,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 WARN Shell: Interrupted while joining on: Thread[Thread-106445,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 123 (task 6998, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 123 (task 6998, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 123 (task 6998, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 125 (task 7000, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 125 (task 7000, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 125 (task 7000, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 138.0 in stage 72.0 (TID 7013) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 123.0 in stage 72.0 (TID 6998) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 125.0 in stage 72.0 (TID 7000) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 126 (task 7001, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 126 (task 7001, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 126 (task 7001, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 126.0 in stage 72.0 (TID 7001) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 128 (task 7003, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 128 (task 7003, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 128 (task 7003, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 128.0 in stage 72.0 (TID 7003) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 139.0 in stage 72.0 (TID 7014) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 127 (task 7002, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 129 (task 7004, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 127 (task 7002, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 129 (task 7004, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 129 (task 7004, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 127 (task 7002, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 129.0 in stage 72.0 (TID 7004) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 127.0 in stage 72.0 (TID 7002) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 7 into HDFSStateStore[id=(op=0,part=131),dir=file:/tmp/temporary-e2be3795-aa89-4d14-b319-b0a2144362fb/state/0/131/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e2be3795-aa89-4d14-b319-b0a2144362fb/state/0/131/right-keyWithIndexToValue/.7.delta.cf2d4d12-d7cd-42da-aec5-7884e2d58f05.TID7006.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 25 more\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 131 (task 7006, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 131 (task 7006, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 131.0 in stage 72.0 (TID 7006) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 130 (task 7005, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 130 (task 7005, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 130 (task 7005, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 130.0 in stage 72.0 (TID 7005) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 133 (task 7008, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 132 (task 7007, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 133 (task 7008, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 132 (task 7007, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 132 (task 7007, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 133 (task 7008, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 132.0 in stage 72.0 (TID 7007) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 133.0 in stage 72.0 (TID 7008) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 134 (task 7009, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 134 (task 7009, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 134 (task 7009, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 134.0 in stage 72.0 (TID 7009) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 135 (task 7010, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 135 (task 7010, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 135 (task 7010, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 135.0 in stage 72.0 (TID 7010) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 136 (task 7011, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 136 (task 7011, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 136 (task 7011, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 136.0 in stage 72.0 (TID 7011) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n",
      "21/12/09 22:06:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 137 (task 7012, attempt 0, stage 72.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborting commit for partition 137 (task 7012, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 ERROR DataWritingSparkTask: Aborted commit for partition 137 (task 7012, attempt 0, stage 72.0)\n",
      "21/12/09 22:06:25 WARN TaskSetManager: Lost task 137.0 in stage 72.0 (TID 7012) (75205fb84683 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    config(\"spark.sql.streaming.schemaInference\", True).\\\n",
    "    config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True).\\\n",
    "    getOrCreate()  \n",
    "\n",
    "\n",
    "stream = spark.readStream.\\\n",
    "    format(\"ws\").\\\n",
    "    option(\"schema\", \"ticker\").\\\n",
    "    load()\n",
    "\n",
    "\n",
    "stream_sell = stream.select(\"side\", \"time\", \"product_id\", \"price\").\\\n",
    "    filter(col(\"side\")==\"sell\")\n",
    "\n",
    "\n",
    "\n",
    "stream_buy = stream.select(\"side\", \"time\", \"product_id\", \"price\").\\\n",
    "    filter(col(\"side\")==\"buy\")\n",
    "\n",
    "\n",
    "# query = stream_sell.\\\n",
    "#     writeStream.\\\n",
    "#     outputMode(\"append\").\\\n",
    "#     format(\"console\").\\\n",
    "#     option(\"truncate\", \"false\").\\\n",
    "#     start()\n",
    "\n",
    "query = stream_sell.\\\n",
    "    alias(\"stream_sell\").\\\n",
    "    join(stream_buy.alias(\"stream_buy\"),\n",
    "        expr(\"\"\"\n",
    "        stream_sell.product_id = stream_buy.product_id AND\n",
    "        (stream_sell.time - stream_buy.time) <= interval 1 seconds AND\n",
    "        (stream_buy.time - stream_sell.time) <= interval 1 seconds\n",
    "        \"\"\")).\\\n",
    "    writeStream.\\\n",
    "    outputMode(\"append\").\\\n",
    "    format(\"console\").\\\n",
    "    option(\"truncate\", \"false\").\\\n",
    "    start()\n",
    "\n",
    "\n",
    "\n",
    "query.awaitTermination(20) \n",
    "query.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
