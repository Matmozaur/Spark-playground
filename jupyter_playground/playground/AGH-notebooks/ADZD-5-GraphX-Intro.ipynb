{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytać graf Facebooka, krawędzie z pliku musae_facebook_edges.csv, atrybuty page_name oraz page_type z pliku musae_facebook_target.csv.  Policzyć liczbę krawędzi i wierzchołków. (1 p.)\n",
    "\n",
    "Sprawdzić czy graf jest spójny. Czy dwa podgrafy utworzone dla typów strony politician oraz company też są spójne? (1 p.)\n",
    "\n",
    "Spośród 1000 stron o najwyższym PageRank znaleźć 50 takich (wypisać page_name i page_type), które mają najmniej połączeń oraz 50 o największej liczbie połączeń. Który typ strony był dominujący w każdej z tych kategorii? Narysować wykres zależności PageRank od liczby krawędzi dla wierzchołków (scatter plot) (2 p.)\n",
    "\n",
    "Korzystając z Pregel API zaimplementować następujący algorytm. W pierwszym kroku wybrana strona publikuje post fake news. W kolejnym kroku ten post publikowany jest przez ⅓ losowo wybranych kontaktów tej strony. W dalszych krokach, dla każdej strony, która opublikowała już ten post, losowo wybrane ⅓ jej kontaktów publikuje go u siebie. Pokazać jak zmienia się liczba stron które opublikowały post w zależności od liczby kroków. (2 p.)\n",
    "Opcjonalnie: Sprawdzić powyższą zależność dla współczynnika innego niż 1/3\n",
    "\n",
    "Narysować wykres rozkładu stopnia wierzchołków w grafie w skali logarytmicznej. Można skorzystać z funkcji obliczającej histogram dla RDD. Czy sieć jest bezskalowa (scale-free)? https://barabasi.com/f/623.pdf  (2 p.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful documentation:\n",
    "\n",
    " - https://spark.apache.org/docs/latest/graphx-programming-guide.html\n",
    " - https://spark.apache.org/docs/latest/api/java/org/apache/spark/graphx/Pregel.html\n",
    " - https://docs.databricks.com/spark/latest/graph-analysis/graph-analysis-graphx-tutorial.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Scala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://25a5e6072dd7:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1606501377774)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 0\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x:Int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 0\n"
     ]
    }
   ],
   "source": [
    "println(\"x = \" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graphs\n",
    "Add some vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myVertices: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[0] at makeRDD at <console>:28\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myVertices = sc.makeRDD(Array(\n",
    "    (1L, \"Alice\"), \n",
    "    (2L, \"Bob\"),\n",
    "    (3L, \"Charlie\"), \n",
    "    (4L, \"John Doe\"), \n",
    "    (5L, \"Eve\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add edges with attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myEdges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at makeRDD at <console>:28\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myEdges = sc.makeRDD(Array(\n",
    "    Edge(1L, 2L, \"is-friends-with\"),\n",
    "    Edge(2L, 3L, \"is-friends-with\"), \n",
    "    Edge(3L, 4L, \"is-friends-with\"),\n",
    "    Edge(4L, 5L, \"follows\"), \n",
    "    Edge(3L, 5L, \"follows\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@13ca2429\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myGraph = Graph(myVertices, myEdges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at vertices, edges and triplets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(org.apache.spark.graphx.VertexId, String)] = Array((1,Alice), (2,Bob), (3,Charlie), (4,John Doe), (5,Eve))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGraph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(1,2,is-friends-with), Edge(2,3,is-friends-with), Edge(3,4,is-friends-with), Edge(4,5,follows), Edge(3,5,follows))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGraph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[org.apache.spark.graphx.EdgeTriplet[String,String]] = Array(((1,Alice),(2,Bob),is-friends-with), ((2,Bob),(3,Charlie),is-friends-with), ((3,Charlie),(4,John Doe),is-friends-with), ((4,John Doe),(5,Eve),follows), ((3,Charlie),(5,Eve),follows))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGraph.triplets.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all triplets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice is-friends-with Bob\n",
      "Bob is-friends-with Charlie\n",
      "Charlie is-friends-with John Doe\n",
      "John Doe follows Eve\n",
      "Charlie follows Eve\n"
     ]
    }
   ],
   "source": [
    "myGraph.triplets.map(\n",
    "  triplet => triplet.srcAttr + \" \" + triplet.attr + \" \" + triplet.dstAttr\n",
    ").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple graph analysis\n",
    "Compute pagerank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myPageRankGraph: org.apache.spark.graphx.Graph[Double,Double] = org.apache.spark.graphx.impl.GraphImpl@208643d1\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myPageRankGraph = myGraph.pageRank(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(org.apache.spark.graphx.VertexId, Double)] = Array((1,0.4390416708169825), (2,0.8122270910114175), (3,1.1294346981766874), (4,0.9190514175420748), (5,1.7002451224528383))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPageRankGraph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(0.4390416708169825,Alice))\n",
      "(2,(0.8122270910114175,Bob))\n",
      "(3,(1.1294346981766874,Charlie))\n",
      "(4,(0.9190514175420748,John Doe))\n",
      "(5,(1.7002451224528383,Eve))\n"
     ]
    }
   ],
   "source": [
    "myGraph.pageRank(0.0001).vertices.join(myVertices).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 has 2 in degrees.\n",
      "4 has 1 in degrees.\n",
      "3 has 1 in degrees.\n",
      "2 has 1 in degrees.\n"
     ]
    }
   ],
   "source": [
    "myGraph\n",
    "  .inDegrees // computes in Degrees\n",
    "  .foreach(x => println(x._1 + \" has \" + x._2 + \" in degrees.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregel program example\n",
    "\n",
    "Goal: collect all the names of friends and followers in all the vertices. \n",
    "In every step all vertices send their names to all their connections, and merge together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the graph such that all vertices start with empty set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialGraph: org.apache.spark.graphx.Graph[scala.collection.immutable.Set[String],String] = org.apache.spark.graphx.impl.GraphImpl@77a58762\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initialGraph = myGraph.mapVertices((_,v) => Set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Set[String])] = Array((1,Set(Alice)), (2,Set(Bob)), (3,Set(Charlie)), (4,Set(John Doe)), (5,Set(Eve)))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialGraph.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Pregel for 1 step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g: org.apache.spark.graphx.Graph[scala.collection.immutable.Set[String],String] = org.apache.spark.graphx.impl.GraphImpl@36aed62b\n",
       "res9: Array[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Set[String])] = Array((1,Set(Alice)), (2,Set(Bob, Alice)), (3,Set(Charlie, Bob)), (4,Set(John Doe, Charlie)), (5,Set(Eve, John Doe, Charlie)))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g = Pregel(initialGraph,                                   //  Graph<VD,ED> graph\n",
    "               Set[String](),                                  //  A initialMsg\n",
    "               1,                                              //  int maxIterations,\n",
    "               activeDirection = EdgeDirection.Out)(           //  EdgeDirection activeDirection,\n",
    "               (id, value, message) => value union message,    //  scala.Function3<Object,VD,A,VD> vprog\n",
    "               triplet =>                                      //  scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg,\n",
    "                    Iterator((triplet.dstId, triplet.srcAttr)),\n",
    "               (a,b) => a union b                              //  mergeMsg\n",
    "              )              \n",
    "\n",
    "g.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Pregel for 2 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g: org.apache.spark.graphx.Graph[scala.collection.immutable.Set[String],String] = org.apache.spark.graphx.impl.GraphImpl@55b0465e\n",
       "res10: Array[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Set[String])] = Array((1,Set(Alice)), (2,Set(Bob, Alice)), (3,Set(Charlie, Bob, Alice)), (4,Set(John Doe, Charlie, Bob)), (5,Set(Eve, John Doe, Charlie, Bob)))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g = Pregel(initialGraph,                                   //  Graph<VD,ED> graph\n",
    "               Set[String](),                                  //  A initialMsg\n",
    "               2,                                              //  int maxIterations,\n",
    "               activeDirection = EdgeDirection.Out)(           //  EdgeDirection activeDirection,\n",
    "               (id, value, message) => value union message,    //  scala.Function3<Object,VD,A,VD> vprog\n",
    "               triplet =>                                      //  scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg,\n",
    "                    Iterator((triplet.dstId, triplet.srcAttr)),\n",
    "               (a,b) => a union b                              //  mergeMsg\n",
    "              )              \n",
    "\n",
    "g.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Pregel for 3 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g: org.apache.spark.graphx.Graph[scala.collection.immutable.Set[String],String] = org.apache.spark.graphx.impl.GraphImpl@65eb27e6\n",
       "res11: Array[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Set[String])] = Array((1,Set(Alice)), (2,Set(Bob, Alice)), (3,Set(Charlie, Bob, Alice)), (4,Set(John Doe, Charlie, Bob, Alice)), (5,Set(Eve, Bob, Alice, John Doe, Charlie)))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g = Pregel(initialGraph,                                   //  Graph<VD,ED> graph\n",
    "               Set[String](),                                  //  A initialMsg\n",
    "               3,                                              //  int maxIterations,\n",
    "               activeDirection = EdgeDirection.Out)(           //  EdgeDirection activeDirection,\n",
    "               (id, value, message) => value union message,    //  scala.Function3<Object,VD,A,VD> vprog\n",
    "               triplet =>                                      //  scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg,\n",
    "                    Iterator((triplet.dstId, triplet.srcAttr)),\n",
    "               (a,b) => a union b                              //  mergeMsg\n",
    "              )              \n",
    "\n",
    "g.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but with more explicit type definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vprog: (id: org.apache.spark.graphx.VertexId, value: Set[String], message: Set[String])Set[String]\n",
       "sendMsg: (et: org.apache.spark.graphx.EdgeTriplet[Set[String],String])Iterator[(org.apache.spark.graphx.VertexId, Set[String])]\n",
       "mergeMsg: (a: Set[String], b: Set[String])Set[String]\n",
       "g: org.apache.spark.graphx.Graph[scala.collection.immutable.Set[String],String] = org.apache.spark.graphx.impl.GraphImpl@7d07f9e9\n",
       "res12: Array[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Set[String])] = Array((1,Set(Alice)), (2,Set(Bob, Alice)), (3,Set(Charlie, Bob, Alice)), (4,Set(John Doe, Charlie, Bob, Alice)), (5,Set(Eve, Bob, Alice, John Doe, Charlie)))\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Vertex program - a function to apply to each vertex on a received message\n",
    "def vprog(id:VertexId, value:Set[String], message:Set[String]): Set[String] = {\n",
    "    value union message\n",
    "}\n",
    "\n",
    "// Send function: \n",
    "def sendMsg(et:EdgeTriplet[Set[String],String]): Iterator[Tuple2[VertexId,Set[String]]] = {                 \n",
    "                    Iterator((et.dstId, et.srcAttr))\n",
    "}\n",
    "\n",
    "def mergeMsg(a:Set[String],b:Set[String]): Set[String] = {\n",
    "    a union b\n",
    "} \n",
    "\n",
    "val g = Pregel(initialGraph,                                 //  Graph<VD,ED> graph\n",
    "               Set[String](),                                //  A initialMsg\n",
    "               4,                                            //  int maxIterations,\n",
    "               activeDirection = EdgeDirection.Out)(         //  EdgeDirection activeDirection,\n",
    "               vprog,                                        //  scala.Function3<Object,VD,A,VD> vprog\n",
    "               sendMsg ,                                     //  scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg,\n",
    "               mergeMsg                                      //  mergeMsg: (A, A) => A\n",
    "              )              \n",
    "\n",
    "g.vertices.collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
